{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install transformers\n",
    "# !python -m pip install datasets==2.21.0\n",
    "# !python -m pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei516/anaconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers as T\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW, Adam, RMSprop\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import SpearmanCorrCoef, Accuracy, F1Score\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有些中文的標點符號在tokenizer編碼以後會變成[UNK]，所以將其換成英文標點\n",
    "token_replacement = [\n",
    "    [\"：\" , \":\"],\n",
    "    [\"，\" , \",\"],\n",
    "    [\"“\" , \"\\\"\"],\n",
    "    [\"”\" , \"\\\"\"],\n",
    "    [\"？\" , \"?\"],\n",
    "    [\"……\" , \"...\"],\n",
    "    [\"！\" , \"!\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset example: \n",
      "{'sentence_pair_id': 1, 'premise': 'A group of kids is playing in a yard and an old man is standing in the background', 'hypothesis': 'A group of boys in a yard is playing and a man is standing in the background', 'relatedness_score': 4.5, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 2, 'premise': 'A group of children is playing in the house and there is no man standing in the background', 'hypothesis': 'A group of kids is playing in a yard and an old man is standing in the background', 'relatedness_score': 3.200000047683716, 'entailment_judgment': 0} \n",
      "{'sentence_pair_id': 3, 'premise': 'The young boys are playing outdoors and the man is smiling nearby', 'hypothesis': 'The kids are playing outdoors near a man with a smile', 'relatedness_score': 4.699999809265137, 'entailment_judgment': 1}\n"
     ]
    }
   ],
   "source": [
    "class SemevalDataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"validation\", \"test\"]\n",
    "        self.data = load_dataset(\n",
    "            \"sem_eval_2014_task_1\", split=split, cache_dir=\"./cache/\"\n",
    "        ).to_list()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data[index]\n",
    "        # 把中文標點替換掉\n",
    "        for k in [\"premise\", \"hypothesis\"]:\n",
    "            for tok in token_replacement:\n",
    "                d[k] = d[k].replace(tok[0], tok[1])\n",
    "        return d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "data_sample = SemevalDataset(split=\"train\").data[:3]\n",
    "print(f\"Dataset example: \\n{data_sample[0]} \\n{data_sample[1]} \\n{data_sample[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "lr = 1e-5\n",
    "epochs = 6\n",
    "train_batch_size = 8\n",
    "validation_batch_size = 8\n",
    "test_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO1: Create batched data for DataLoader\n",
    "# `collate_fn` is a function that defines how the data batch should be packed.\n",
    "# This function will be called in the DataLoader to pack the data batch.\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO1-1: Implement the collate_fn function\n",
    "    # The input parameter is a data batch (tuple), and this function packs it into tensors.\n",
    "    # Use tokenizer to pack tokenize and pack the data and its corresponding labels.\n",
    "    # Return the data batch and labels for each sub-task.\n",
    "\n",
    "    # Preprocess data\n",
    "    input_texts = [f\"{item['premise']} [SEP] {item['hypothesis']}\" for item in batch]\n",
    "    # Tokenize\n",
    "    input_encodings = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # To tensors\n",
    "    labels1 = torch.tensor([item['relatedness_score'] for item in batch])\n",
    "    labels2 = torch.tensor([item['entailment_judgment'] for item in batch])\n",
    "\n",
    "    return input_encodings, labels1, labels2\n",
    "\n",
    "# TODO1-2: Define your DataLoader\n",
    "dl_train = DataLoader(SemevalDataset(split=\"train\"), batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dl_validation = DataLoader(SemevalDataset(split=\"validation\"), batch_size=validation_batch_size, collate_fn=collate_fn)\n",
    "dl_test = DataLoader(SemevalDataset(split=\"test\"), batch_size=test_batch_size, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO2: Construct your model\n",
    "class MultiLabelModel(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Define what modules you will use in the model\n",
    "        self.bert = T.BertModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        # 3-Class Classification\n",
    "        self.classifier = torch.nn.Linear(hidden_size, 3)\n",
    "\n",
    "        # Regression\n",
    "        self.regressor = torch.nn.Linear(hidden_size, 1)\n",
    "        \"\"\"\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.LayerNorm(hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_size, hidden_size // 2),\n",
    "            torch.nn.LayerNorm(hidden_size // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # Extract BERT features\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # Apply linear layers\n",
    "        relatedness = self.regressor(pooled_output)\n",
    "        entailment = self.classifier(pooled_output)\n",
    "        return relatedness, entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLabelModel().to(device)\n",
    "tokenizer = T.BertTokenizer.from_pretrained(model_name, cache_dir=\"./cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(next(iter(dl_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei516/anaconda3/envs/nlp/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "# TODO3: Define your optimizer and loss function\n",
    "\n",
    "# TODO3-1: Define your Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "# optimizer = RMSprop(model.parameters(), lr=lr)\n",
    "# optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# TODO3-2: Define your loss functions (you should have two)\n",
    "loss_classifier = torch.nn.CrossEntropyLoss()\n",
    "loss_regressor = torch.nn.MSELoss()\n",
    "\n",
    "# scoring functions\n",
    "spc = SpearmanCorrCoef()\n",
    "acc = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=3, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/6]: 100%|████████████████████████████████████████████████| 563/563 [00:29<00:00, 19.40it/s, loss=1.16]\n",
      "Validation epoch [1/6]: 100%|██████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 102.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Corr: 0.771\n",
      "Accuracy: 0.816\n",
      "F1 Score: 0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [2/6]: 100%|███████████████████████████████████████████████| 563/563 [00:28<00:00, 19.53it/s, loss=0.348]\n",
      "Validation epoch [2/6]: 100%|██████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 101.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Corr: 0.807\n",
      "Accuracy: 0.862\n",
      "F1 Score: 0.860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [3/6]: 100%|██████████████████████████████████████████████| 563/563 [00:28<00:00, 19.54it/s, loss=0.0788]\n",
      "Validation epoch [3/6]: 100%|██████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 102.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Corr: 0.819\n",
      "Accuracy: 0.836\n",
      "F1 Score: 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [4/6]: 100%|███████████████████████████████████████████████| 563/563 [00:28<00:00, 19.65it/s, loss=0.666]\n",
      "Validation epoch [4/6]: 100%|██████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 102.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Corr: 0.830\n",
      "Accuracy: 0.866\n",
      "F1 Score: 0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [5/6]: 100%|██████████████████████████████████████████████| 563/563 [00:28<00:00, 19.70it/s, loss=0.0852]\n",
      "Validation epoch [5/6]: 100%|██████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 102.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Corr: 0.831\n",
      "Accuracy: 0.862\n",
      "F1 Score: 0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [6/6]: 100%|███████████████████████████████████████████████| 563/563 [00:28<00:00, 19.62it/s, loss=0.167]\n",
      "Validation epoch [6/6]: 100%|██████████████████████████████████████████████████████████| 63/63 [00:00<00:00, 103.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Corr: 0.841\n",
      "Accuracy: 0.882\n",
      "F1 Score: 0.880\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "alpha = 0.9 \n",
    "smoothed_loss_reg = None\n",
    "smoothed_loss_clf = None\n",
    "threshold = 0.5\n",
    "\n",
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "    model.train()\n",
    "    # TODO4: Write the training loop\n",
    "\n",
    "    # Train\n",
    "    for batch in pbar:\n",
    "        # clear gradient\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # forward pass\n",
    "        input_encodings, labels1, labels2 = batch\n",
    "        input_ids = input_encodings.input_ids.to(device)\n",
    "        attention_mask = input_encodings.attention_mask.to(device)\n",
    "        token_type_ids = input_encodings.token_type_ids.to(device)\n",
    "        labels1 = labels1.to(device)\n",
    "        labels2 = labels2.to(device)\n",
    "        relatedness, entailment = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        # compute loss\n",
    "        loss_reg = loss_regressor(relatedness.squeeze(), labels1)\n",
    "        loss_clf = loss_classifier(entailment, labels2)        \n",
    "        loss = loss_reg + loss_clf\n",
    " \n",
    "        \"\"\"\n",
    "        (1) Directly Add\n",
    "        loss_reg = loss_regressor(relatedness.squeeze(), labels1)\n",
    "        loss_clf = loss_classifier(entailment, labels2)\n",
    "        loss = loss_reg + loss_clf\n",
    "        (2)\n",
    "        # Prevent divided by zero error\n",
    "        weight_reg = 1 / (loss_reg.item() + 1e-8)\n",
    "        weight_clf = 1 / (loss_clf.item() + 1e-8)\n",
    "        total_weight = weight_reg + weight_clf\n",
    "        loss = (weight_reg / total_weight) * loss_reg + (weight_clf / total_weight) * loss_clf\n",
    "        (3)\n",
    "        loss_reg = loss_regressor(relatedness.squeeze(), labels1)\n",
    "        loss_clf = loss_classifier(entailment, labels2)\n",
    "        scaled_loss_reg = task_weights[0] * loss_reg\n",
    "        scaled_loss_clf = task_weights[1] * loss_clf\n",
    "        loss = scaled_loss_reg + scaled_loss_clf\n",
    "\n",
    "        grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
    "        grads_norms = [torch.norm(g.detach(), p=2) for g in grads if g is not None]\n",
    "        norm_reg = grads_norms[0] / grads_norms[0].item()\n",
    "        norm_clf = grads_norms[1] / grads_norms[1].item()\n",
    "\n",
    "        target_grad = torch.tensor([1.0, 1.0], device=device)\n",
    "        lambda_reg = task_weights[0].data * (norm_reg / target_grad[0])\n",
    "        lambda_clf = task_weights[1].data * (norm_clf / target_grad[1])\n",
    "        new_task_weights = torch.tensor([lambda_reg, lambda_clf], device=device)\n",
    "        task_weights.data = new_task_weights / new_task_weights.sum()\n",
    "        (4)\n",
    "        loss_reg = loss_regressor(relatedness.squeeze(), labels1)\n",
    "        loss_clf = loss_classifier(entailment, labels2)        \n",
    "        if ep % 2 == 0:\n",
    "            loss = loss_reg\n",
    "        else:\n",
    "            loss = loss_clf\n",
    "        (5)\n",
    "        loss_reg = loss_regressor(relatedness.squeeze(), labels1)\n",
    "        loss_clf = loss_classifier(entailment, labels2)        \n",
    "        \n",
    "        if smoothed_loss_reg is None:\n",
    "            smoothed_loss_reg = loss_reg\n",
    "            smoothed_loss_clf = loss_clf\n",
    "        else:\n",
    "            #smoothed_loss = alpha * loss.item() + (1 - alpha) * smoothed_loss\n",
    "            smoothed_loss_reg = (1 - alpha) * loss_reg + alpha * smoothed_loss_reg.detach()\n",
    "            smoothed_loss_clf = (1 - alpha) * loss_clf + alpha * smoothed_loss_clf.detach()\n",
    "        \n",
    "        loss = smoothed_loss_reg + smoothed_loss_clf   \n",
    "        (6)\n",
    "        loss_reg = loss_regressor(relatedness.squeeze(), labels1)\n",
    "        loss_clf = loss_classifier(entailment, labels2)        \n",
    "\n",
    "        loss = loss_reg * 0.6 + loss_clf * 0.4  \n",
    "        \"\"\"\n",
    "        \n",
    "        # back-propagation\n",
    "        loss.backward()\n",
    "    \n",
    "        # model optimization\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    # TODO5: Write the evaluation loop\n",
    "    # Evaluate\n",
    "    with torch.no_grad():\n",
    "        relatedness_preds = []\n",
    "        entailment_preds = []\n",
    "        relatedness_labels = []\n",
    "        entailment_labels = []\n",
    "        \n",
    "        #incorrect_samples = []\n",
    "        \n",
    "        for batch in pbar:\n",
    "            input_encodings, labels1, labels2 = batch\n",
    "            input_ids = input_encodings.input_ids.to(device)\n",
    "            attention_mask = input_encodings.attention_mask.to(device)\n",
    "            token_type_ids = input_encodings.token_type_ids.to(device)\n",
    "            labels1 = labels1.to(device)\n",
    "            labels2 = labels2.to(device)\n",
    "            rel, ent = model(input_ids, attention_mask, token_type_ids)\n",
    "            \n",
    "            #entailment_preds = ent.argmax(dim=1)\n",
    "            #entailment_errors = entailment_preds != labels2\n",
    "            #incorrect_samples += [(input_encodings, labels1[i], labels2[i], rel[i], entailment_preds[i]) for i in range(len(labels2)) if entailment_errors[i]]\n",
    "            #regression_errors = torch.abs(rel.squeeze() - labels1) > threshold  # 定義偏差閾值\n",
    "            #incorrect_samples += [(input_encodings, labels1[i], labels2[i], rel[i], entailment_preds[i]) for i in range(len(labels1)) if regression_errors[i]]\n",
    "\n",
    "            relatedness_preds.extend(rel.squeeze().tolist())\n",
    "            entailment_preds.extend(ent.argmax(dim=1).tolist())\n",
    "            relatedness_labels.extend(labels1.tolist())\n",
    "            entailment_labels.extend(labels2.tolist())\n",
    "        \n",
    "        spc_score = spc(torch.tensor(relatedness_preds), torch.tensor(relatedness_labels))\n",
    "        acc_score = acc(torch.tensor(entailment_preds), torch.tensor(entailment_labels))\n",
    "        f1_score = f1(torch.tensor(entailment_preds), torch.tensor(entailment_labels))\n",
    "        \n",
    "        # Output all the evaluation scores (SpearmanCorrCoef, Accuracy, F1Score)\n",
    "        print(f\"Spearman Corr: {spc_score:.3f}\")\n",
    "        print(f\"Accuracy: {acc_score:.3f}\")\n",
    "        print(f\"F1 Score: {f1_score:.3f}\")\n",
    "        \n",
    "        torch.save(incorrect_samples, \"incorrect_samples.pt\")    \n",
    "    torch.save(model, f'./saved_models/ep{ep}.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For test set predictions, you can write perform evaluation simlar to #TODO5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test set evaluation: 100%|███████████████████████████████████████████████████████████| 616/616 [00:05<00:00, 105.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set - Spearman Corr: 0.828 | Accuracy: 0.877 | F1 Score: 0.869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(dl_test)\n",
    "pbar.set_description(f\"Test set evaluation\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    relatedness_preds = []\n",
    "    entailment_preds = []\n",
    "    relatedness_labels = []\n",
    "    entailment_labels = []\n",
    "    for batch in pbar:\n",
    "        input_encodings, labels1, labels2 = batch\n",
    "        input_ids = input_encodings.input_ids.to(device)\n",
    "        attention_mask = input_encodings.attention_mask.to(device)\n",
    "        token_type_ids = input_encodings.token_type_ids.to(device)\n",
    "        labels1 = labels1.to(device)\n",
    "        labels2 = labels2.to(device)\n",
    "        rel, ent = model(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "        relatedness_preds.extend(rel.squeeze().tolist())\n",
    "        entailment_preds.extend(ent.argmax(dim=1).tolist())\n",
    "        relatedness_labels.extend(labels1.tolist())\n",
    "        entailment_labels.extend(labels2.tolist())\n",
    "        \n",
    "    spc_score = spc(torch.tensor(relatedness_preds), torch.tensor(relatedness_labels))\n",
    "    acc_score = acc(torch.tensor(entailment_preds), torch.tensor(entailment_labels))\n",
    "    f1_score = f1(torch.tensor(entailment_preds), torch.tensor(entailment_labels))\n",
    "    \n",
    "    print(f\"Test set - Spearman Corr: {spc_score:.3f} | Accuracy: {acc_score:.3f} | F1 Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test set evaluation: 100%|████████████████████████████████████████████████████████████| 616/616 [00:08<00:00, 76.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set - Spearman Corr: 0.828 | Accuracy: 0.877 | F1 Score: 0.869\n",
      "Total errors logged: 4448\n",
      "Errors containing 'no': 559/4448 (12.57%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "pbar = tqdm(dl_test)\n",
    "pbar.set_description(\"Test set evaluation\")\n",
    "model.eval()\n",
    "\n",
    "error_log = defaultdict(list)\n",
    "no_count = 0  \n",
    "total_error_count = 0 \n",
    "\n",
    "with torch.no_grad():\n",
    "    relatedness_preds = []\n",
    "    entailment_preds = []\n",
    "    relatedness_labels = []\n",
    "    entailment_labels = []\n",
    "\n",
    "    for idx, batch in enumerate(pbar):\n",
    "        input_encodings, labels1, labels2 = batch\n",
    "        input_ids = input_encodings.input_ids.to(device)\n",
    "        attention_mask = input_encodings.attention_mask.to(device)\n",
    "        token_type_ids = input_encodings.token_type_ids.to(device)\n",
    "        labels1 = labels1.to(device)\n",
    "        labels2 = labels2.to(device)\n",
    "\n",
    "        rel, ent = model(input_ids, attention_mask, token_type_ids)\n",
    "        \n",
    "        relatedness_preds_batch = rel.squeeze().tolist()\n",
    "        entailment_preds_batch = ent.argmax(dim=1).tolist()\n",
    "        relatedness_labels_batch = labels1.tolist()\n",
    "        entailment_labels_batch = labels2.tolist()\n",
    "\n",
    "        relatedness_preds.extend(relatedness_preds_batch)\n",
    "        entailment_preds.extend(entailment_preds_batch)\n",
    "        relatedness_labels.extend(relatedness_labels_batch)\n",
    "        entailment_labels.extend(entailment_labels_batch)\n",
    "        \n",
    "        for i in range(len(relatedness_labels_batch)):\n",
    "            if (round(relatedness_preds_batch[i]) != relatedness_labels_batch[i]) or \\\n",
    "               (entailment_preds_batch[i] != entailment_labels_batch[i]):\n",
    "                total_error_count += 1  \n",
    "                input_ids_list = input_encodings.input_ids[i].tolist()\n",
    "                decoded_text = tokenizer.decode(input_ids_list, skip_special_tokens=True)\n",
    "\n",
    "                if re.search(r'\\bno\\b', decoded_text, re.IGNORECASE):\n",
    "                    no_count += 1\n",
    "\n",
    "                error_log[\"batch_index\"].append(idx)\n",
    "                error_log[\"data_index\"].append(i)\n",
    "                error_log[\"input_text\"].append(decoded_text)\n",
    "                error_log[\"true_relatedness\"].append(relatedness_labels_batch[i])\n",
    "                error_log[\"pred_relatedness\"].append(relatedness_preds_batch[i])\n",
    "                error_log[\"true_entailment\"].append(entailment_labels_batch[i])\n",
    "                error_log[\"pred_entailment\"].append(entailment_preds_batch[i])\n",
    "\n",
    "    spc_score = spc(torch.tensor(relatedness_preds), torch.tensor(relatedness_labels))\n",
    "    acc_score = acc(torch.tensor(entailment_preds), torch.tensor(entailment_labels))\n",
    "    f1_score = f1(torch.tensor(entailment_preds), torch.tensor(entailment_labels))\n",
    "    \n",
    "    print(f\"Test set - Spearman Corr: {spc_score:.3f} | Accuracy: {acc_score:.3f} | F1 Score: {f1_score:.3f}\")\n",
    "\n",
    "print(f\"Total errors logged: {total_error_count}\")\n",
    "print(f\"Errors containing 'no': {no_count}/{total_error_count} ({(no_count / total_error_count) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 2254:\n",
      "  Batch Index: 311\n",
      "  Data Index: 6\n",
      "  Input Text: a man is loading a rifle with bullets a woman is seasoning a piece of meat\n",
      "  True Relatedness: 1.100000023841858, Predicted: 0.98\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 3829:\n",
      "  Batch Index: 526\n",
      "  Data Index: 3\n",
      "  Input Text: a large dog and a small dog are standing next to the kitchen counter and are sniffing a large dog and a small dog are standing next to the kitchen counter and are investigating\n",
      "  True Relatedness: 4.300000190734863, Predicted: 4.35\n",
      "  True Entailment: 1, Predicted: 0\n",
      "Error 1547:\n",
      "  Batch Index: 214\n",
      "  Data Index: 1\n",
      "  Input Text: a woman is freeing a fish a man is catching a fish\n",
      "  True Relatedness: 2.5, Predicted: 2.67\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 2030:\n",
      "  Batch Index: 280\n",
      "  Data Index: 5\n",
      "  Input Text: a woman is riding a horse there is no woman riding a horse\n",
      "  True Relatedness: 3.299999952316284, Predicted: 3.59\n",
      "  True Entailment: 2, Predicted: 2\n",
      "Error 3671:\n",
      "  Batch Index: 505\n",
      "  Data Index: 6\n",
      "  Input Text: a young man is getting a motocross bike up a dirt hill a young man is pushing a motocross bike up a dirt hill\n",
      "  True Relatedness: 4.599999904632568, Predicted: 4.29\n",
      "  True Entailment: 1, Predicted: 0\n",
      "Error 3368:\n",
      "  Batch Index: 464\n",
      "  Data Index: 5\n",
      "  Input Text: some men are playing rugby the bunch of men are playing with the mud on a rugby field\n",
      "  True Relatedness: 3.799999952316284, Predicted: 3.39\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 3727:\n",
      "  Batch Index: 513\n",
      "  Data Index: 1\n",
      "  Input Text: a boy is jumping on the side of a stone water fountain in front of a building a boy is jumping off the side of a stone water fountain in front of a building\n",
      "  True Relatedness: 4.199999809265137, Predicted: 4.86\n",
      "  True Entailment: 2, Predicted: 1\n",
      "Error 3210:\n",
      "  Batch Index: 443\n",
      "  Data Index: 1\n",
      "  Input Text: a ball is being caught by a dog in mid air a dog is catching a ball in mid air\n",
      "  True Relatedness: 4.900000095367432, Predicted: 4.74\n",
      "  True Entailment: 1, Predicted: 1\n",
      "Error 2951:\n",
      "  Batch Index: 408\n",
      "  Data Index: 0\n",
      "  Input Text: the group of people is sitting on the ground outside a monument a group of people is sitting on both sides of a red stone structure\n",
      "  True Relatedness: 3.5999999046325684, Predicted: 3.37\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 3622:\n",
      "  Batch Index: 499\n",
      "  Data Index: 2\n",
      "  Input Text: a girl with red hair and red eyebrows is in midspeech the girl has red hair and eyebrows, several piercings in a ear and a tattoo on the back\n",
      "  True Relatedness: 3.200000047683716, Predicted: 3.56\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 2519:\n",
      "  Batch Index: 348\n",
      "  Data Index: 4\n",
      "  Input Text: a man is dancing on the floor a famous singer is dancing on the ceiling\n",
      "  True Relatedness: 2.799999952316284, Predicted: 2.92\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 4352:\n",
      "  Batch Index: 596\n",
      "  Data Index: 5\n",
      "  Input Text: a girl in an orange shirt and clown makeup is walking in a park and others are looking on a girl is wearing an orange shirt and a striped tie\n",
      "  True Relatedness: 3.799999952316284, Predicted: 3.43\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 1774:\n",
      "  Batch Index: 244\n",
      "  Data Index: 1\n",
      "  Input Text: the men are fist fighting in a ring the men are not fist fighting in a ring\n",
      "  True Relatedness: 3.5, Predicted: 3.59\n",
      "  True Entailment: 2, Predicted: 2\n",
      "Error 3197:\n",
      "  Batch Index: 441\n",
      "  Data Index: 3\n",
      "  Input Text: a man is standing next to a bus a man is standing on a sidewalk\n",
      "  True Relatedness: 3.4000000953674316, Predicted: 2.86\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 336:\n",
      "  Batch Index: 45\n",
      "  Data Index: 0\n",
      "  Input Text: a dog, which is black, and a white one are staring at each other in the street along two streets, different breeds of dogs are being looked at\n",
      "  True Relatedness: 2.9000000953674316, Predicted: 3.82\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 1306:\n",
      "  Batch Index: 180\n",
      "  Data Index: 5\n",
      "  Input Text: a woman is skillfully playing a flute the woman is playing the flute\n",
      "  True Relatedness: 4.599999904632568, Predicted: 4.91\n",
      "  True Entailment: 1, Predicted: 1\n",
      "Error 3127:\n",
      "  Batch Index: 432\n",
      "  Data Index: 2\n",
      "  Input Text: a black dog with a red collar is putting its face in the water a dog with a black coat is standing in shallow water\n",
      "  True Relatedness: 2.799999952316284, Predicted: 3.50\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 1903:\n",
      "  Batch Index: 261\n",
      "  Data Index: 4\n",
      "  Input Text: there is no man playing an electronic keyboard a man is playing an electronic keyboard\n",
      "  True Relatedness: 3.200000047683716, Predicted: 3.63\n",
      "  True Entailment: 2, Predicted: 2\n",
      "Error 3562:\n",
      "  Batch Index: 490\n",
      "  Data Index: 6\n",
      "  Input Text: a man dressed in black and white is holding up the tennis racket and is waiting for the ball a man dressed in black and white is dropping the tennis racket and is waiting for the ball\n",
      "  True Relatedness: 4.0, Predicted: 3.88\n",
      "  True Entailment: 2, Predicted: 0\n",
      "Error 2743:\n",
      "  Batch Index: 379\n",
      "  Data Index: 1\n",
      "  Input Text: a brown dog is jumping in the air a brown dog is sitting down\n",
      "  True Relatedness: 2.5, Predicted: 3.39\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 4119:\n",
      "  Batch Index: 565\n",
      "  Data Index: 6\n",
      "  Input Text: a lady is standing on the street and is surrounded by school children a lady is standing on the street and is surrounded by kids\n",
      "  True Relatedness: 4.599999904632568, Predicted: 4.88\n",
      "  True Entailment: 1, Predicted: 1\n",
      "Error 3116:\n",
      "  Batch Index: 430\n",
      "  Data Index: 6\n",
      "  Input Text: orange juice is being drunk by a man who is walking on a sunny day a man is drinking orange juice and walking\n",
      "  True Relatedness: 4.400000095367432, Predicted: 4.66\n",
      "  True Entailment: 1, Predicted: 1\n",
      "Error 1726:\n",
      "  Batch Index: 237\n",
      "  Data Index: 5\n",
      "  Input Text: a woman is slicing a pepper which is green the woman with a knife is slicing a pepper\n",
      "  True Relatedness: 4.400000095367432, Predicted: 4.07\n",
      "  True Entailment: 1, Predicted: 0\n",
      "Error 1386:\n",
      "  Batch Index: 191\n",
      "  Data Index: 7\n",
      "  Input Text: broccoli is being chopped by a woman a woman is chopping broccoli\n",
      "  True Relatedness: 4.900000095367432, Predicted: 5.07\n",
      "  True Entailment: 1, Predicted: 1\n",
      "Error 4351:\n",
      "  Batch Index: 596\n",
      "  Data Index: 4\n",
      "  Input Text: a boy is wearing an orange shirt and a striped tie a girl in an orange shirt and clown makeup is standing in a park and others are looking on\n",
      "  True Relatedness: 1.600000023841858, Predicted: 2.50\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 3868:\n",
      "  Batch Index: 531\n",
      "  Data Index: 4\n",
      "  Input Text: rollerbladers are rolling in formation down a hill rollerbladers are running in formation down a hill\n",
      "  True Relatedness: 4.199999809265137, Predicted: 4.53\n",
      "  True Entailment: 0, Predicted: 1\n",
      "Error 3405:\n",
      "  Batch Index: 470\n",
      "  Data Index: 1\n",
      "  Input Text: a man is performing a handstand on the beach the man at the beach is not doing a handstand on the wet sand at the edge of the water\n",
      "  True Relatedness: 4.300000190734863, Predicted: 3.54\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 3449:\n",
      "  Batch Index: 475\n",
      "  Data Index: 6\n",
      "  Input Text: a surfer is riding a big wave across dark green water the big surfer is riding and waving\n",
      "  True Relatedness: 3.700000047683716, Predicted: 4.12\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 1549:\n",
      "  Batch Index: 214\n",
      "  Data Index: 3\n",
      "  Input Text: a man is catching a fish a woman is deboning a fish\n",
      "  True Relatedness: 3.4000000953674316, Predicted: 1.69\n",
      "  True Entailment: 0, Predicted: 0\n",
      "Error 3621:\n",
      "  Batch Index: 499\n",
      "  Data Index: 1\n",
      "  Input Text: the girl has red hair and eyebrows, several piercings in a ear and a tattoo on the back a girl with red hair and red eyebrows is talking\n",
      "  True Relatedness: 3.299999952316284, Predicted: 3.66\n",
      "  True Entailment: 0, Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "num_to_print = min(30, len(error_log['batch_index']))\n",
    "random_indices = random.sample(range(len(error_log['batch_index'])), num_to_print)\n",
    "\n",
    "for idx in random_indices:\n",
    "    print(f\"Error {idx + 1}:\")\n",
    "    print(f\"  Batch Index: {error_log['batch_index'][idx]}\")\n",
    "    print(f\"  Data Index: {error_log['data_index'][idx]}\")\n",
    "    print(f\"  Input Text: {error_log['input_text'][idx]}\")\n",
    "    print(f\"  True Relatedness: {error_log['true_relatedness'][idx]}, Predicted: {error_log['pred_relatedness'][idx]:.2f}\")\n",
    "    print(f\"  True Entailment: {error_log['true_entailment'][idx]}, Predicted: {error_log['pred_entailment'][idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
